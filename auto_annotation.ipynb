{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "2d71a4ad",
   "metadata": {},
   "source": [
    "[![Labellerr](https://storage.googleapis.com/labellerr-cdn/%200%20Labellerr%20template/notebook.webp)](https://www.labellerr.com)\n",
    "\n",
    "# **SAM3 Batch Inference**\n",
    "\n",
    "---\n",
    "\n",
    "[![labellerr](https://img.shields.io/badge/Labellerr-BLOG-black.svg)](https://www.labellerr.com/blog)\n",
    "[![Youtube](https://img.shields.io/badge/Labellerr-YouTube-b31b1b.svg)](https://www.youtube.com/@Labellerr)\n",
    "[![Github](https://img.shields.io/badge/Labellerr-GitHub-green.svg)](https://github.com/Labellerr/Hands-On-Learning-in-Computer-Vision)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ddf0990d",
   "metadata": {},
   "source": [
    "Perform batch inference on images using SAM3 (Segment Anything Model v3) with text prompts and export annotations in COCO-JSON format. Seamlessly upload results to Labellerr for review and refinement."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eee5178e",
   "metadata": {},
   "source": [
    "## **Quickstart**\n",
    "\n",
    "1. **Install dependencies** â€” Follow the [README.md](README.md) to set up SAM3 and Labellerr SDK\n",
    "\n",
    "2. **Download SAM3 model** â€” Get `sam3.pt` from [Hugging Face](https://huggingface.co/facebook/sam3) and place it in the `model/` folder\n",
    "\n",
    "3. **Configure inference** â€” Set `INPUT_FOLDER` (your images), `MODEL_CHECKPOINT`, and `TEXT_PROMPTS` (objects to detect)\n",
    "\n",
    "4. **Run batch inference** â€” Execute `run_batch_inference()` to generate COCO-JSON annotations in `SAM3_Results/`\n",
    "\n",
    "5. **Upload to Labellerr** â€” Set your `PROJECT_ID` and run `upload_preannotations()` to review annotations on [Labellerr](https://www.labellerr.com)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dc4074eb",
   "metadata": {},
   "source": [
    "## **INSTALLATION**\n",
    "\n",
    "Run the below cell and follow the instructions in the [README.md](README.md)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "78a2546c",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Cloning into 'yolo_finetune_utils'...\n"
     ]
    }
   ],
   "source": [
    "# uncomment the following lines if you have not cloned the repository and installed the SDK\n",
    "# !git clone https://github.com/facebookresearch/sam3.git\n",
    "# %pip install git+https://github.com/Labellerr/SDKPython.git\n",
    "# !git clone https://github.com/yashsuman15/yolo_finetune_utils.git"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7496ee99",
   "metadata": {},
   "source": [
    "## **1. Download SAM3 Model from Huggingface**\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "583eb473",
   "metadata": {},
   "source": [
    "### SAM3 Batch Inference: Model Download Instructions\n",
    "\n",
    "To perform SAM3 batch inference, you need to download the SAM3 model from the Hugging Face model hub.\n",
    "\n",
    "**SAM 3 (Segment Anything Model v3)** is developed by Meta and currently requires access permission.\n",
    "\n",
    "### Steps to get the SAM3 Model:\n",
    "\n",
    "1.  **Step 1:** Visit the official [SAM 3 repository on Hugging Face](https://huggingface.co/facebook/sam3).\n",
    "2.  **Step 2:** Request access (log in to Hugging Face and fill out the access form).\n",
    "3.  **Step 3:** Once youâ€™re approved, download the model checkpoint from files and versions (e.g., `sam3.pt`) in the designated model folder."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "000d294d",
   "metadata": {},
   "source": [
    "## **Import Required functions**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "b34e7eb2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# This function is responsible for executing the batch inference process using the SAM3 model,\n",
    "from sam3_batch_inference import run_batch_inference\n",
    "\n",
    "# This function is designed to handle the batch uploading of pre-generated annotations to a Labellerr project.\n",
    "from batch_upload_preannot import upload_preannotations"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4c38b1e1",
   "metadata": {},
   "source": [
    "## **Extract Frame from the videos**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "938413e9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[âœ“] Extracted 150 frames to folder: traffic_dataset_frames\n"
     ]
    }
   ],
   "source": [
    "from yolo_finetune_utils.frame_extractor import extract_random_frames\n",
    "\n",
    "extract_random_frames(\n",
    "        paths=[r\"videos\\traffic_dataset\"],\n",
    "        total_images=150,\n",
    "        out_dir=\"traffic_dataset_frames\",\n",
    "        jpg_quality=100,\n",
    "        seed=42\n",
    "    )"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "80f232d5",
   "metadata": {},
   "source": [
    "## **2. Fill the parameters for SAM3 batch inference**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "3c68df1b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================================\n",
    "# CONFIGURATION - Fill this values\n",
    "# ============================================================\n",
    "\n",
    "# Path to the folder containing input images.\n",
    "# Example: \"flower_sample_img\"\n",
    "INPUT_FOLDER = \"traffic_dataset_frames\"\n",
    "\n",
    "# Path to the pre-trained model checkpoint file (.pt).\n",
    "# This file contains the weights for the SAM3 model.\n",
    "# Example: r\"model/sam3.pt\"\n",
    "MODEL_CHECKPOINT = \"model/sam3.pt\"\n",
    "\n",
    "# A list of text prompts to be used for segmenting objects within the images.\n",
    "# Each string in the list represents a different object or class to detect.\n",
    "# Example: [\"Red Flower\", \"Yellow Flower\", \"White Flower\", \"Violet Flower\"]\n",
    "TEXT_PROMPTS = [\"vehicle number plate\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "ea25a7e7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading SAM3 model from: model/sam3.pt\n",
      "Model loaded successfully\n",
      "Found 150 images to process\n",
      "Text prompts (1): ['vehicle number plate']\n",
      "Confidence threshold: 0.4\n",
      "Segmentation format: polygon\n",
      "Processing strategy: Prompt-sequential (memory optimized)\n",
      "\n",
      "============================================================\n",
      "Processing prompt 1/1: 'vehicle number plate'\n",
      "============================================================\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[vehicle number plate]: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 150/150 [04:05<00:00,  1.64s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "============================================================\n",
      "Processing complete!\n",
      "============================================================\n",
      "Total images processed: 150\n",
      "Total categories: 1\n",
      "Total annotations: 562\n",
      "Average detections per image: 3.75\n",
      "\n",
      "Per-category breakdown:\n",
      "  - vehicle number plate: 562 detections\n",
      "\n",
      "Output saved to: SAM3_Results/traffic_dataset_frames/annotations.json\n",
      "============================================================\n"
     ]
    }
   ],
   "source": [
    "# Call the batch inference function with specified parameters.\n",
    "run_batch_inference(\n",
    "    INPUT_FOLDER,       # Path to the input folder containing images for inference.\n",
    "    TEXT_PROMPTS,       # List of text prompts to guide the segmentation model.\n",
    "    MODEL_CHECKPOINT,   # Path to the pre-trained model checkpoint file (.pt).\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "34f325a3",
   "metadata": {},
   "source": [
    "## **3. Review SAM3 annotations on Labellerr**\n",
    "\n",
    "Here's how to set up your environment for reviewing SAM3 annotations:\n",
    "\n",
    "1.  [Go to Labellerr](https://www.labellerr.com/?utm_source=githubY&utm_medium=social&utm_campaign=github_clicks) and create a dataset on labellerr with your image folder.\n",
    "\n",
    "2.  Create a project from that labellerr dataset. Copy the `project_id` and paste it into the `PROJECT_ID` variable.\n",
    "\n",
    "![Labellerr](https://cdn.labellerr.com/1%20%20Documentation/1c9dc7ce-9a54-4111-8fd5-0363ba3e00e1.webp)\n",
    "\n",
    "3.  From the Labellerr API tab [check the docs](https://docs.labellerr.com/sdk/getting-started#getting-started), copy the `API Key`, `API SECRET`, and `CLIENT ID`. Then, create a `.env` file and paste the values into it as follows: \n",
    "\n",
    "    ```\n",
    "    API_KEY = \"\"\n",
    "    API_SECRET = \"\"\n",
    "    CLIENT_ID = \"\"\n",
    "    ```\n",
    "\n",
    "    **NOTE:** Ensure the `.env` file is in the same directory as this notebook, and the variable names match the strings shown above exactly.\n",
    "4.  Copy the path of the result annotations folder (where annotations are stored) and paste it into the `ANNOTATION_DIR` variable."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "ee5ff785",
   "metadata": {},
   "outputs": [],
   "source": [
    "PROJECT_ID = \"sileas_historical_badger_69255\"\n",
    "ANNOTATION_DIR = \"SAM3_Results/traffic_dataset_frames\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "4e9fd3d1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "============================================================\n",
      "Batch Upload Started\n",
      "============================================================\n",
      "Project ID: sileas_historical_badger_69255\n",
      "Annotation Format: coco_json\n",
      "Total batch files: 1\n",
      "Delay between uploads: 2s\n",
      "============================================================\n",
      "\n",
      "[1/1] Uploading: annotations.json\n",
      "  âœ“ Success! Metadata: {'files_not_updated': [], 'activity_id': '5b46eb1f-4e0f-4cf1-82d0-d238a5b918f7', 'questions_ignored': []}\n",
      "\n",
      "============================================================\n",
      "Batch Upload Complete!\n",
      "============================================================\n",
      "Total batches: 1\n",
      "Successful uploads: 1\n",
      "Failed uploads: 0\n",
      "Total time: 109.72s\n",
      "============================================================\n"
     ]
    }
   ],
   "source": [
    "result = upload_preannotations(\n",
    "    project_id=PROJECT_ID,\n",
    "    annotation_format='coco_json',\n",
    "    batch_annotation_dir=ANNOTATION_DIR\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c67162bf",
   "metadata": {},
   "source": [
    "### When Pre-Annotations are loaded, you can now go to [Labellerr UI](https://www.labellerr.com/?utm_source=githubY&utm_medium=social&utm_campaign=github_clicks) to review and fix the annotations"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8390bae1",
   "metadata": {},
   "source": [
    "## **4. Export your annotations from Labellerr**\n",
    "\n",
    "![Labellerr](https://storage.googleapis.com/labellerr-cdn/%200%20Documentation-template/export_guide.webp)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "063ac2cc",
   "metadata": {},
   "source": [
    "## **5. Train Models with Exported COCO-JSON on Our CV Notebooks**\n",
    "\n",
    "Check out these hands-on notebooks to train YOLO models for your specific use case:\n",
    "\n",
    "- **[Fine-Tune your custom data on YOLOv12](https://github.com/Labellerr/Hands-On-Learning-in-Computer-Vision/blob/main/fine-tune%20YOLO%20for%20various%20use%20cases/Pill_Counting_Using_YOLOv12.ipynb)**  \n",
    "  Build an automated pill counting system for pharmaceutical and healthcare applications.\n",
    "\n",
    "- **[Intrusion Detection Using YOLOv8](https://github.com/Labellerr/Hands-On-Learning-in-Computer-Vision/blob/main/fine-tune%20YOLO%20for%20various%20use%20cases/Intrusion_detection_using_YOLO.ipynb)**  \n",
    "  Create a security system that detects unauthorized access in real-time.\n",
    "\n",
    "- **[Fine-Tune RT-DETR for Object Detection](https://github.com/Labellerr/Hands-On-Learning-in-Computer-Vision/blob/main/fine-tune%20YOLO%20for%20various%20use%20cases/Oil_Field_Monitoring_using_CV.ipynb)**  \n",
    "  Monitor and analyze vehicle traffic patterns for smart city applications.\n",
    "\n",
    "- **[Cell Counting with YOLO](https://github.com/Labellerr/Hands-On-Learning-in-Computer-Vision/blob/main/fine-tune%20YOLO%20for%20various%20use%20cases/Fine-Tune-YOLO-for-Cell-Counting.ipynb)**  \n",
    "  Automate biological cell counting for medical research and diagnostics.\n",
    "\n",
    "---\n",
    "\n",
    "## ðŸ§‘â€ðŸ”¬ Check Our Videos tutorials for Training\n",
    "\n",
    "Whether you're a beginner or a practitioner, our hands-on training videos are perfect for learning custom model building, computer vision techniques, and applied AI:\n",
    "\n",
    "- [How to Fine-Tune YOLO on Custom Dataset](https://www.youtube.com/watch?v=pBLWOe01QXU)  \n",
    "  Step-by-step guide to fine-tuning YOLO for real-world useâ€”environment setup, annotation, training, validation, and inference.\n",
    "- [Build a Real-Time Intrusion Detection System with YOLO](https://www.youtube.com/watch?v=kwQeokYDVcE)  \n",
    "  Create an AI-powered system to detect intruders in real time using YOLO and computer vision.\n",
    "- [Finding Athlete Speed Using YOLO](https://www.youtube.com/watch?v=txW0CQe_pw0)  \n",
    "  Estimate real-time speed of athletes for sports analytics.\n",
    "- [Object Counting Using AI](https://www.youtube.com/watch?v=smsjBBQcIUQ)  \n",
    "  Learn dataset curation, annotation, and training for robust object counting AI applications.\n",
    "\n",
    "---\n",
    "\n",
    "## ðŸ¤ Stay Connected\n",
    "\n",
    "- **Website:** [https://www.labellerr.com/](https://www.labellerr.com/)\n",
    "- **Blog:** [https://www.labellerr.com/blog/](https://www.labellerr.com/blog/)\n",
    "- **GitHub:** [Labellerr/Hands-On-Learning-in-Computer-Vision](https://github.com/Labellerr/Hands-On-Learning-in-Computer-Vision)\n",
    "- **LinkedIn:** [Labellerr](https://in.linkedin.com/company/labellerr)\n",
    "- **Twitter/X:** [@Labellerr1](https://x.com/Labellerr1)\n",
    "\n",
    "*Happy learning and building with Labellerr!*"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
