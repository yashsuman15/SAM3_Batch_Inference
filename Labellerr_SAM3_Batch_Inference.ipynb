{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "2d71a4ad",
   "metadata": {},
   "source": [
    "[![Labellerr](https://storage.googleapis.com/labellerr-cdn/%200%20Labellerr%20template/notebook.webp)](https://www.labellerr.com)\n",
    "\n",
    "# **SAM3 Batch Inference**\n",
    "\n",
    "---\n",
    "\n",
    "[![labellerr](https://img.shields.io/badge/Labellerr-BLOG-black.svg)](https://www.labellerr.com/blog)\n",
    "[![Youtube](https://img.shields.io/badge/Labellerr-YouTube-b31b1b.svg)](https://www.youtube.com/@Labellerr)\n",
    "[![Github](https://img.shields.io/badge/Labellerr-GitHub-green.svg)](https://github.com/Labellerr/Hands-On-Learning-in-Computer-Vision)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ddf0990d",
   "metadata": {},
   "source": [
    "Perform batch inference on images using SAM3 (Segment Anything Model v3) with text prompts and export annotations in COCO-JSON format. Seamlessly upload results to Labellerr for review and refinement."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dc4074eb",
   "metadata": {},
   "source": [
    "## **INSTALLATION**\n",
    "\n",
    "Run the below cell and follow the instructions in the [README.md](README.md)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "78a2546c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# !git clone https://github.com/facebookresearch/sam3.git\n",
    "# %pip install git+https://github.com/Labellerr/SDKPython.gitQ"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7496ee99",
   "metadata": {},
   "source": [
    "## **Download SAM3 Model from Huggingface**\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "583eb473",
   "metadata": {},
   "source": [
    "### SAM3 Batch Inference: Model Download Instructions\n",
    "\n",
    "To perform SAM3 batch inference, you need to download the SAM3 model from the Hugging Face model hub.\n",
    "\n",
    "**SAM 3 (Segment Anything Model v3)** is developed by Meta and currently requires access permission.\n",
    "\n",
    "### Steps to get the SAM3 Model:\n",
    "\n",
    "1.  **Step 1:** Visit the official [SAM 3 repository on Hugging Face](https://huggingface.co/facebook/sam3).\n",
    "2.  **Step 2:** Request access (log in to Hugging Face and fill out the access form).\n",
    "3.  **Step 3:** Once you’re approved, download the model checkpoint from files and versions (e.g., `sam3.pt`) in the designated model folder."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "000d294d",
   "metadata": {},
   "source": [
    "## **Import Required functions**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "b34e7eb2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# This function is responsible for executing the batch inference process using the SAM3 model,\n",
    "from sam3_batch_inference import run_batch_inference\n",
    "\n",
    "# This function is designed to handle the batch uploading of pre-generated annotations to a Labellerr project.\n",
    "from batch_upload_preannot import upload_preannotations"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "80f232d5",
   "metadata": {},
   "source": [
    "## **Fill the parameters for SAM3 batch inference**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "3c68df1b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================================\n",
    "# CONFIGURATION - Fill this values\n",
    "# ============================================================\n",
    "\n",
    "# Path to the folder containing input images.\n",
    "# Example: \"flower_sample_img\"\n",
    "INPUT_FOLDER = \"flower_sample_img\"\n",
    "\n",
    "# Path to the pre-trained model checkpoint file (.pt).\n",
    "# This file contains the weights for the SAM3 model.\n",
    "# Example: r\"model\\sam3.pt\"\n",
    "MODEL_CHECKPOINT = r\"model\\sam3.pt\"\n",
    "\n",
    "# A list of text prompts to be used for segmenting objects within the images.\n",
    "# Each string in the list represents a different object or class to detect.\n",
    "# Example: [\"Red Flower\", \"Yellow Flower\", \"White Flower\", \"Violet Flower\"]\n",
    "TEXT_PROMPTS = [\"Red Flower\", \n",
    "                \"Yellow Flower\", \n",
    "                \"White Flower\",\n",
    "                \"Violet Flower\",\n",
    "                ]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "ea25a7e7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading SAM3 model from: model\\sam3.pt\n",
      "Model loaded successfully\n",
      "Found 7 images to process\n",
      "Text prompts (4): ['Red Flower', 'Yellow Flower', 'White Flower', 'Violet Flower']\n",
      "Confidence threshold: 0.4\n",
      "Segmentation format: polygon\n",
      "Processing strategy: Prompt-sequential (memory optimized)\n",
      "\n",
      "============================================================\n",
      "Processing prompt 1/4: 'Red Flower'\n",
      "============================================================\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Red Flower]: 100%|██████████| 7/7 [00:27<00:00,  3.97s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "============================================================\n",
      "Processing prompt 2/4: 'Yellow Flower'\n",
      "============================================================\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Yellow Flower]: 100%|██████████| 7/7 [00:20<00:00,  2.90s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "============================================================\n",
      "Processing prompt 3/4: 'White Flower'\n",
      "============================================================\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[White Flower]: 100%|██████████| 7/7 [00:28<00:00,  4.09s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "============================================================\n",
      "Processing prompt 4/4: 'Violet Flower'\n",
      "============================================================\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Violet Flower]: 100%|██████████| 7/7 [00:12<00:00,  1.71s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "============================================================\n",
      "Processing complete!\n",
      "============================================================\n",
      "Total images processed: 7\n",
      "Total categories: 4\n",
      "Total annotations: 402\n",
      "Average detections per image: 57.43\n",
      "\n",
      "Per-category breakdown:\n",
      "  - Red Flower: 131 detections\n",
      "  - Yellow Flower: 102 detections\n",
      "  - White Flower: 155 detections\n",
      "  - Violet Flower: 14 detections\n",
      "\n",
      "Output saved to: SAM3_Results/flower_sample_img/annotations.json\n",
      "============================================================\n"
     ]
    }
   ],
   "source": [
    "# Call the batch inference function with specified parameters.\n",
    "run_batch_inference(\n",
    "    INPUT_FOLDER,       # Path to the input folder containing images for inference.\n",
    "    TEXT_PROMPTS,       # List of text prompts to guide the segmentation model.\n",
    "    MODEL_CHECKPOINT,   # Path to the pre-trained model checkpoint file (.pth).\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "34f325a3",
   "metadata": {},
   "source": [
    "## **Review SAM3 annotations on Labellerr**\n",
    "\n",
    "Here's how to set up your environment for reviewing SAM3 annotations:\n",
    "\n",
    "1.  Go to Labellerr and create a dataset with your image folder.\n",
    "2.  Create a project from that dataset. Copy the `project_id` and paste it into the `PROJECT_ID` variable.\n",
    "3.  From the Labellerr API tab, copy the `API Key`, `API SECRET`, and `CLIENT ID`. Then, create a `.env` file and paste the values into it as follows:\n",
    "\n",
    "    ```\n",
    "    API_KEY = \"\"\n",
    "    API_SECRET = \"\"\n",
    "    CLIENT_ID = \"\"\n",
    "    ```\n",
    "\n",
    "    **NOTE:** Ensure the `.env` file is in the same directory as this notebook, and the variable names match the strings shown above exactly.\n",
    "4.  Copy the path of the result annotations folder (where annotations are stored) and paste it into the `ANNOTATION_DIR` variable."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "ee5ff785",
   "metadata": {},
   "outputs": [],
   "source": [
    "PROJECT_ID = \"lynelle_additional_gibbon_85467\"\n",
    "ANNOTATION_DIR = r\"SAM3_Results\\flower_sample_img\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "4e9fd3d1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "============================================================\n",
      "Batch Upload Started\n",
      "============================================================\n",
      "Project ID: lynelle_additional_gibbon_85467\n",
      "Annotation Format: coco_json\n",
      "Total batch files: 1\n",
      "Delay between uploads: 2s\n",
      "============================================================\n",
      "\n",
      "[1/1] Uploading: annotations.json\n",
      "  ✓ Success! Metadata: {'activity_id': '4d44bcc7-55bb-45ba-8e26-418f5dd5699e', 'files_not_updated': [], 'questions_ignored': []}\n",
      "\n",
      "============================================================\n",
      "Batch Upload Complete!\n",
      "============================================================\n",
      "Total batches: 1\n",
      "Successful uploads: 1\n",
      "Failed uploads: 0\n",
      "Total time: 98.27s\n",
      "============================================================\n"
     ]
    }
   ],
   "source": [
    "result = upload_preannotations(\n",
    "    project_id=PROJECT_ID,\n",
    "    annotation_format='coco_json',\n",
    "    batch_annotation_dir=ANNOTATION_DIR\n",
    ")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
