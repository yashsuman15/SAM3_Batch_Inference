{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "2d71a4ad",
   "metadata": {},
   "source": [
    "[![Labellerr](https://storage.googleapis.com/labellerr-cdn/%200%20Labellerr%20template/notebook.webp)](https://www.labellerr.com)\n",
    "\n",
    "# **SAM3 Batch Inference**\n",
    "\n",
    "---\n",
    "\n",
    "[![labellerr](https://img.shields.io/badge/Labellerr-BLOG-black.svg)](https://www.labellerr.com/blog)\n",
    "[![Youtube](https://img.shields.io/badge/Labellerr-YouTube-b31b1b.svg)](https://www.youtube.com/@Labellerr)\n",
    "[![Github](https://img.shields.io/badge/Labellerr-GitHub-green.svg)](https://github.com/Labellerr/Hands-On-Learning-in-Computer-Vision)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ddf0990d",
   "metadata": {},
   "source": [
    "Perform batch inference on images using SAM3 (Segment Anything Model v3) with text prompts and export annotations in COCO-JSON format. Seamlessly upload results to Labellerr for review and refinement."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dc4074eb",
   "metadata": {},
   "source": [
    "## **INSTALLATION**\n",
    "\n",
    "Run the below cell and follow the instructions in the [README.md](README.md)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "78a2546c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# !git clone https://github.com/facebookresearch/sam3.git\n",
    "# %pip install git+https://github.com/Labellerr/SDKPython.gitQ"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7496ee99",
   "metadata": {},
   "source": [
    "## **1. Download SAM3 Model from Huggingface**\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "583eb473",
   "metadata": {},
   "source": [
    "### SAM3 Batch Inference: Model Download Instructions\n",
    "\n",
    "To perform SAM3 batch inference, you need to download the SAM3 model from the Hugging Face model hub.\n",
    "\n",
    "**SAM 3 (Segment Anything Model v3)** is developed by Meta and currently requires access permission.\n",
    "\n",
    "### Steps to get the SAM3 Model:\n",
    "\n",
    "1.  **Step 1:** Visit the official [SAM 3 repository on Hugging Face](https://huggingface.co/facebook/sam3).\n",
    "2.  **Step 2:** Request access (log in to Hugging Face and fill out the access form).\n",
    "3.  **Step 3:** Once youâ€™re approved, download the model checkpoint from files and versions (e.g., `sam3.pt`) in the designated model folder."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "000d294d",
   "metadata": {},
   "source": [
    "## **Import Required functions**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "b34e7eb2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# This function is responsible for executing the batch inference process using the SAM3 model,\n",
    "from sam3_batch_inference import run_batch_inference\n",
    "\n",
    "# This function is designed to handle the batch uploading of pre-generated annotations to a Labellerr project.\n",
    "from batch_upload_preannot import upload_preannotations"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "80f232d5",
   "metadata": {},
   "source": [
    "## **2. Fill the parameters for SAM3 batch inference**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "3c68df1b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================================\n",
    "# CONFIGURATION - Fill this values\n",
    "# ============================================================\n",
    "\n",
    "# Path to the folder containing input images.\n",
    "# Example: \"flower_sample_img\"\n",
    "INPUT_FOLDER = \"flower_sample_img\"\n",
    "\n",
    "# Path to the pre-trained model checkpoint file (.pt).\n",
    "# This file contains the weights for the SAM3 model.\n",
    "# Example: r\"model\\sam3.pt\"\n",
    "MODEL_CHECKPOINT = r\"model\\sam3.pt\"\n",
    "\n",
    "# A list of text prompts to be used for segmenting objects within the images.\n",
    "# Each string in the list represents a different object or class to detect.\n",
    "# Example: [\"Red Flower\", \"Yellow Flower\", \"White Flower\", \"Violet Flower\"]\n",
    "TEXT_PROMPTS = [\"Red Flower\", \n",
    "                \"Yellow Flower\", \n",
    "                \"White Flower\",\n",
    "                \"Violet Flower\",\n",
    "                ]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "ea25a7e7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading SAM3 model from: model\\sam3.pt\n",
      "Model loaded successfully\n",
      "Found 7 images to process\n",
      "Text prompts (4): ['Red Flower', 'Yellow Flower', 'White Flower', 'Violet Flower']\n",
      "Confidence threshold: 0.4\n",
      "Segmentation format: polygon\n",
      "Processing strategy: Prompt-sequential (memory optimized)\n",
      "\n",
      "============================================================\n",
      "Processing prompt 1/4: 'Red Flower'\n",
      "============================================================\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Red Flower]: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 7/7 [00:27<00:00,  3.97s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "============================================================\n",
      "Processing prompt 2/4: 'Yellow Flower'\n",
      "============================================================\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Yellow Flower]: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 7/7 [00:20<00:00,  2.90s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "============================================================\n",
      "Processing prompt 3/4: 'White Flower'\n",
      "============================================================\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[White Flower]: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 7/7 [00:28<00:00,  4.09s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "============================================================\n",
      "Processing prompt 4/4: 'Violet Flower'\n",
      "============================================================\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Violet Flower]: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 7/7 [00:12<00:00,  1.71s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "============================================================\n",
      "Processing complete!\n",
      "============================================================\n",
      "Total images processed: 7\n",
      "Total categories: 4\n",
      "Total annotations: 402\n",
      "Average detections per image: 57.43\n",
      "\n",
      "Per-category breakdown:\n",
      "  - Red Flower: 131 detections\n",
      "  - Yellow Flower: 102 detections\n",
      "  - White Flower: 155 detections\n",
      "  - Violet Flower: 14 detections\n",
      "\n",
      "Output saved to: SAM3_Results/flower_sample_img/annotations.json\n",
      "============================================================\n"
     ]
    }
   ],
   "source": [
    "# Call the batch inference function with specified parameters.\n",
    "run_batch_inference(\n",
    "    INPUT_FOLDER,       # Path to the input folder containing images for inference.\n",
    "    TEXT_PROMPTS,       # List of text prompts to guide the segmentation model.\n",
    "    MODEL_CHECKPOINT,   # Path to the pre-trained model checkpoint file (.pth).\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "34f325a3",
   "metadata": {},
   "source": [
    "## **3. Review SAM3 annotations on Labellerr**\n",
    "\n",
    "Here's how to set up your environment for reviewing SAM3 annotations:\n",
    "\n",
    "1.  [Go to Labellerr](https://www.labellerr.com/?utm_source=githubY&utm_medium=social&utm_campaign=github_clicks) and create a dataset on labellerr with your image folder.\n",
    "\n",
    "2.  Create a project from that labellerr dataset. Copy the `project_id` and paste it into the `PROJECT_ID` variable.\n",
    "\n",
    "![Labellerr](https://cdn.labellerr.com/1%20%20Documentation/1c9dc7ce-9a54-4111-8fd5-0363ba3e00e1.webp)\n",
    "\n",
    "3.  From the Labellerr API tab [check the docs](https://docs.labellerr.com/sdk/getting-started#getting-started), copy the `API Key`, `API SECRET`, and `CLIENT ID`. Then, create a `.env` file and paste the values into it as follows: \n",
    "\n",
    "    ```\n",
    "    API_KEY = \"\"\n",
    "    API_SECRET = \"\"\n",
    "    CLIENT_ID = \"\"\n",
    "    ```\n",
    "\n",
    "    **NOTE:** Ensure the `.env` file is in the same directory as this notebook, and the variable names match the strings shown above exactly.\n",
    "4.  Copy the path of the result annotations folder (where annotations are stored) and paste it into the `ANNOTATION_DIR` variable."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "ee5ff785",
   "metadata": {},
   "outputs": [],
   "source": [
    "PROJECT_ID = \"lynelle_additional_gibbon_85467\"\n",
    "ANNOTATION_DIR = r\"SAM3_Results\\flower_sample_img\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "4e9fd3d1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "============================================================\n",
      "Batch Upload Started\n",
      "============================================================\n",
      "Project ID: lynelle_additional_gibbon_85467\n",
      "Annotation Format: coco_json\n",
      "Total batch files: 1\n",
      "Delay between uploads: 2s\n",
      "============================================================\n",
      "\n",
      "[1/1] Uploading: annotations.json\n",
      "  âœ“ Success! Metadata: {'activity_id': '4d44bcc7-55bb-45ba-8e26-418f5dd5699e', 'files_not_updated': [], 'questions_ignored': []}\n",
      "\n",
      "============================================================\n",
      "Batch Upload Complete!\n",
      "============================================================\n",
      "Total batches: 1\n",
      "Successful uploads: 1\n",
      "Failed uploads: 0\n",
      "Total time: 98.27s\n",
      "============================================================\n"
     ]
    }
   ],
   "source": [
    "result = upload_preannotations(\n",
    "    project_id=PROJECT_ID,\n",
    "    annotation_format='coco_json',\n",
    "    batch_annotation_dir=ANNOTATION_DIR\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8390bae1",
   "metadata": {},
   "source": [
    "## **4. Export your annotations from Labellerr**\n",
    "\n",
    "![Labellerr](https://storage.googleapis.com/labellerr-cdn/%200%20Documentation-template/export_guide.webp)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "063ac2cc",
   "metadata": {},
   "source": [
    "## **5. Ready-to-Use CV Training Notebooks for your COCO-JSON**\n",
    "\n",
    "Check out these hands-on notebooks to train YOLO models for your specific use case:\n",
    "\n",
    "- **[Pill Counting Using YOLOv12](https://github.com/Labellerr/Hands-On-Learning-in-Computer-Vision/blob/main/fine-tune%20YOLO%20for%20various%20use%20cases/Pill_Counting_Using_YOLOv12.ipynb)**  \n",
    "  Build an automated pill counting system for pharmaceutical and healthcare applications.\n",
    "\n",
    "- **[Intrusion Detection Using YOLO](https://github.com/Labellerr/Hands-On-Learning-in-Computer-Vision/blob/main/fine-tune%20YOLO%20for%20various%20use%20cases/Intrusion_detection_using_YOLO.ipynb)**  \n",
    "  Create a security system that detects unauthorized access in real-time.\n",
    "\n",
    "- **[Traffic Flow Counting with YOLO](https://github.com/Labellerr/Hands-On-Learning-in-Computer-Vision/blob/main/fine-tune%20YOLO%20for%20various%20use%20cases/Fine-Tune-YOLO-for-Traffic-Flow-Counting.ipynb)**  \n",
    "  Monitor and analyze vehicle traffic patterns for smart city applications.\n",
    "\n",
    "- **[Cell Counting with YOLO](https://github.com/Labellerr/Hands-On-Learning-in-Computer-Vision/blob/main/fine-tune%20YOLO%20for%20various%20use%20cases/Fine-Tune-YOLO-for-Cell-Counting.ipynb)**  \n",
    "  Automate biological cell counting for medical research and diagnostics.\n",
    "\n",
    "---\n",
    "\n",
    "## ðŸ§‘â€ðŸ”¬ Check Our Videos tutorials for Training\n",
    "\n",
    "Whether you're a beginner or a practitioner, our hands-on training videos are perfect for learning custom model building, computer vision techniques, and applied AI:\n",
    "\n",
    "- [How to Fine-Tune YOLO on Custom Dataset](https://www.youtube.com/watch?v=pBLWOe01QXU)  \n",
    "  Step-by-step guide to fine-tuning YOLO for real-world useâ€”environment setup, annotation, training, validation, and inference.\n",
    "- [Build a Real-Time Intrusion Detection System with YOLO](https://www.youtube.com/watch?v=kwQeokYDVcE)  \n",
    "  Create an AI-powered system to detect intruders in real time using YOLO and computer vision.\n",
    "- [Finding Athlete Speed Using YOLO](https://www.youtube.com/watch?v=txW0CQe_pw0)  \n",
    "  Estimate real-time speed of athletes for sports analytics.\n",
    "- [Object Counting Using AI](https://www.youtube.com/watch?v=smsjBBQcIUQ)  \n",
    "  Learn dataset curation, annotation, and training for robust object counting AI applications.\n",
    "\n",
    "---\n",
    "\n",
    "## ðŸ¤ Stay Connected\n",
    "\n",
    "- **Website:** [https://www.labellerr.com/](https://www.labellerr.com/)\n",
    "- **Blog:** [https://www.labellerr.com/blog/](https://www.labellerr.com/blog/)\n",
    "- **GitHub:** [Labellerr/Hands-On-Learning-in-Computer-Vision](https://github.com/Labellerr/Hands-On-Learning-in-Computer-Vision)\n",
    "- **LinkedIn:** [Labellerr](https://in.linkedin.com/company/labellerr)\n",
    "- **Twitter/X:** [@Labellerr1](https://x.com/Labellerr1)\n",
    "\n",
    "*Happy learning and building with Labellerr!*"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
