{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "2d71a4ad",
   "metadata": {},
   "source": [
    "[![Labellerr](https://storage.googleapis.com/labellerr-cdn/%200%20Labellerr%20template/notebook.webp)](https://www.labellerr.com)\n",
    "\n",
    "# **SAM3 Batch Inference**\n",
    "\n",
    "---\n",
    "\n",
    "[![labellerr](https://img.shields.io/badge/Labellerr-BLOG-black.svg)](https://www.labellerr.com/blog)\n",
    "[![Youtube](https://img.shields.io/badge/Labellerr-YouTube-b31b1b.svg)](https://www.youtube.com/@Labellerr)\n",
    "[![Github](https://img.shields.io/badge/Labellerr-GitHub-green.svg)](https://github.com/Labellerr/Hands-On-Learning-in-Computer-Vision)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ddf0990d",
   "metadata": {},
   "source": [
    "Perform batch inference on images using SAM3 (Segment Anything Model v3) with text prompts and export annotations in COCO-JSON format. Seamlessly upload results to Labellerr for review and refinement."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eee5178e",
   "metadata": {},
   "source": [
    "## **Quickstart**\n",
    "\n",
    "1. **Install dependencies** ‚Äî Follow the [README.md](README.md) to set up SAM3 and Labellerr SDK\n",
    "\n",
    "2. **Download SAM3 model** ‚Äî Get `sam3.pt` from [Hugging Face](https://huggingface.co/facebook/sam3) and place it in the `model/` folder\n",
    "\n",
    "3. **Configure inference** ‚Äî Set `INPUT_FOLDER` (your images), `MODEL_CHECKPOINT`, and `TEXT_PROMPTS` (objects to detect)\n",
    "\n",
    "4. **Run batch inference** ‚Äî Execute `run_batch_inference()` to generate COCO-JSON annotations in `SAM3_Results/`\n",
    "\n",
    "5. **Upload to Labellerr** ‚Äî Set your `PROJECT_ID` and run `upload_preannotations()` to review annotations on [Labellerr](https://www.labellerr.com)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dc4074eb",
   "metadata": {},
   "source": [
    "## **INSTALLATION**\n",
    "\n",
    "Run the below cell and follow the instructions in the [README.md](README.md)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "78a2546c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# uncomment the following lines if you have not cloned the repository and installed the SDK\n",
    "# !git clone https://github.com/facebookresearch/sam3.git\n",
    "# %pip install git+https://github.com/Labellerr/SDKPython.git"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7496ee99",
   "metadata": {},
   "source": [
    "## **1. Download SAM3 Model from Huggingface**\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "583eb473",
   "metadata": {},
   "source": [
    "### SAM3 Batch Inference: Model Download Instructions\n",
    "\n",
    "To perform SAM3 batch inference, you need to download the SAM3 model from the Hugging Face model hub.\n",
    "\n",
    "**SAM 3 (Segment Anything Model v3)** is developed by Meta and currently requires access permission.\n",
    "\n",
    "### Steps to get the SAM3 Model:\n",
    "\n",
    "1.  **Step 1:** Visit the official [SAM 3 repository on Hugging Face](https://huggingface.co/facebook/sam3).\n",
    "2.  **Step 2:** Request access (log in to Hugging Face and fill out the access form).\n",
    "3.  **Step 3:** Once you‚Äôre approved, download the model checkpoint from files and versions (e.g., `sam3.pt`) in the designated model folder."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "000d294d",
   "metadata": {},
   "source": [
    "### **Import Required functions**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "b34e7eb2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# This function is responsible for executing the batch inference process using the SAM3 model,\n",
    "from sam3_batch_inference import run_batch_inference\n",
    "\n",
    "# This function is designed to handle the batch uploading of pre-generated annotations to a Labellerr project.\n",
    "from batch_upload_preannot import upload_preannotations"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "80f232d5",
   "metadata": {},
   "source": [
    "## **2. Fill the parameters for SAM3 batch inference**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "3c68df1b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================================\n",
    "# CONFIGURATION - Fill this values\n",
    "# ============================================================\n",
    "\n",
    "# Path to the folder containing input images.\n",
    "# Example: \"flower_sample_img\"\n",
    "INPUT_FOLDER = \"sample_img\"\n",
    "\n",
    "# Path to the pre-trained model checkpoint file (.pt).\n",
    "# This file contains the weights for the SAM3 model.\n",
    "# Example: r\"model/sam3.pt\"\n",
    "MODEL_CHECKPOINT = \"model/sam3.pt\"\n",
    "\n",
    "# A list of text prompts to be used for segmenting objects within the images.\n",
    "# Each string in the list represents a different object or class to detect.\n",
    "# Example: [\"Red Flower\", \"Yellow Flower\", \"White Flower\", \"Violet Flower\"]\n",
    "TEXT_PROMPTS = [\"Car\", \n",
    "                \"Bus\", \n",
    "                \"Person\",\n",
    "                ]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ea25a7e7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Call the batch inference function with specified parameters.\n",
    "run_batch_inference(\n",
    "    INPUT_FOLDER,       # Path to the input folder containing images for inference.\n",
    "    TEXT_PROMPTS,       # List of text prompts to guide the segmentation model.\n",
    "    MODEL_CHECKPOINT,   # Path to the pre-trained model checkpoint file (.pt).\n",
    "    \n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "64db7ee8",
   "metadata": {},
   "source": [
    "## **4. Labellerr Annotation Review Workflow**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "8625d437",
   "metadata": {},
   "outputs": [],
   "source": [
    "from labellerr.client import LabellerrClient\n",
    "from dotenv import load_dotenv\n",
    "import os\n",
    "\n",
    "# For Labellerr Dataset \n",
    "from labellerr.core.datasets import create_dataset_from_local, LabellerrDataset\n",
    "from labellerr.core.schemas import DatasetConfig\n",
    "\n",
    "# For Annotation Template\n",
    "from labellerr.core.annotation_templates import create_template\n",
    "from labellerr.core.schemas.annotation_templates import AnnotationQuestion, QuestionType, CreateTemplateParams, DatasetDataType\n",
    "from template_helper import create_questions_from_prompts\n",
    "import uuid\n",
    "\n",
    "# For labellerr Project\n",
    "from labellerr.core.projects import create_project, LabellerrProject\n",
    "from labellerr.core.schemas.projects import CreateProjectParams, RotationConfig"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e3253c5f",
   "metadata": {},
   "source": [
    "## **Configuration Setup steps**\n",
    "\n",
    "1. [Go to Labellerr](https://www.labellerr.com/?utm_source=githubY&utm_medium=social&utm_campaign=github_clicks) and Login.\n",
    "\n",
    "2. From the Labellerr API tab [check the docs](https://docs.labellerr.com/sdk/getting-started#getting-started), copy the `API Key`, `API SECRET`, and `CLIENT ID`. Then, create a `.env` file and paste the values into it as follows: \n",
    "\n",
    "    ```\n",
    "    API_KEY = \"\"\n",
    "    API_SECRET = \"\"\n",
    "    CLIENT_ID = \"\"\n",
    "    ```\n",
    "\n",
    "    **NOTE:** Ensure the `.env` file is in the same directory as this notebook, and the variable names match the strings shown above exactly.\n",
    "\n",
    "3. Fill the values in the cells below."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1bb82c05",
   "metadata": {},
   "source": [
    "### ***FILL THESE VALUES***"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "26ea6ed7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Copy the path of the result annotations folder (where annotations are stored) and paste it into the `ANNOTATION_DIR` variable.\n",
    "ANNOTATION_DIR = \"SAM3_Results/sample_img\"\n",
    "\n",
    "\n",
    "DATASET_NAME = \"My_Labellerr_Dataset\"\n",
    "TEMPLATE_NAME = \"SDK SAM3 Batch Inference Template\"\n",
    "PROJECT_NAME = \"SDK SAM3 Review Project\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "11465a86",
   "metadata": {},
   "outputs": [],
   "source": [
    "load_dotenv(\".env\")\n",
    "\n",
    "client_id = os.getenv(\"CLIENT_ID\")\n",
    "api_key = os.getenv(\"API_KEY\")\n",
    "api_secret = os.getenv(\"API_SECRET\")\n",
    "\n",
    "if not client_id or not api_key or not api_secret:\n",
    "    raise ValueError(\"CLIENT_ID, API_KEY, and API_SECRET must be set in the .env file\")\n",
    "\n",
    "CLIENT = LabellerrClient(client_id=client_id, \n",
    "                        api_key=api_key, \n",
    "                        api_secret=api_secret)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ba1a7343",
   "metadata": {},
   "source": [
    "### **4.1. Creation Annotation Guideline Based on your Text Prompts**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "f724015b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Template created successfully with ID: 63c14a2c-4594-4df7-b356-1fc9960c255a\n"
     ]
    }
   ],
   "source": [
    "QUESTIONS = create_questions_from_prompts(TEXT_PROMPTS)\n",
    "template = create_template(\n",
    "    client=CLIENT,\n",
    "    params=CreateTemplateParams(\n",
    "        template_name=TEMPLATE_NAME,\n",
    "        data_type=DatasetDataType.image,\n",
    "        questions=QUESTIONS\n",
    "    )\n",
    ")\n",
    "\n",
    "if template.annotation_template_id is not None:\n",
    "    print(f\"Template created successfully with ID: {template.annotation_template_id}\")\n",
    "else:\n",
    "    print(\"Failed to create template\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "253ff2bb",
   "metadata": {},
   "source": [
    "### **4.1. Create a Dataset on labellerr using your local images folder**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1d92c52c",
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = create_dataset_from_local(\n",
    "        client=CLIENT,\n",
    "        dataset_config=DatasetConfig(dataset_name=DATASET_NAME, \n",
    "                                     data_type=\"image\"),\n",
    "        folder_to_upload=INPUT_FOLDER,\n",
    "    )\n",
    "\n",
    "print(\"Waiting for dataset to be processed\")\n",
    "dataset.status()\n",
    "print(\"Dataset processed\")\n",
    "\n",
    "print(\"Dataset ID: \", dataset.dataset_id)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ed265c63",
   "metadata": {},
   "source": [
    "### **4.2. Create a Labellerr Annotation Project using that Dataset**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bcaa48fe",
   "metadata": {},
   "outputs": [],
   "source": [
    "project = create_project(\n",
    "    client=CLIENT,\n",
    "    params=CreateProjectParams(\n",
    "        project_name=PROJECT_NAME,\n",
    "        data_type=DatasetDataType.image,\n",
    "        rotations=RotationConfig(\n",
    "            annotation_rotation_count=1,\n",
    "            review_rotation_count=1,\n",
    "            client_review_rotation_count=1\n",
    "        )\n",
    "    ),\n",
    "    datasets=[dataset],\n",
    "    annotation_template=template\n",
    ")\n",
    "\n",
    "print(\"Created project with ID: \" + project.project_id)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c0a2bdda",
   "metadata": {},
   "source": [
    "### **4.3. Upload SAM3 Pre-Annotations on labellerr**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "4e9fd3d1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "============================================================\n",
      "Batch Upload Started\n",
      "============================================================\n",
      "Project ID: eryn_defeated_pony_99911\n",
      "Annotation Format: coco_json\n",
      "Total batch files: 1\n",
      "Delay between uploads: 2s\n",
      "============================================================\n",
      "\n",
      "[1/1] Uploading: annotations.json\n",
      "  ‚úì Success! Metadata: {'questions_ignored': [], 'activity_id': 'c5c47abf-b37b-4ec9-83ea-05a3df3ae4a6', 'files_not_updated': ['traffic_3_frame_000965_t16.08s_000149.jpg', 'traffic_3_frame_000738_t12.30s_000139.jpg']}\n",
      "\n",
      "============================================================\n",
      "Batch Upload Complete!\n",
      "============================================================\n",
      "Total batches: 1\n",
      "Successful uploads: 1\n",
      "Failed uploads: 0\n",
      "Total time: 113.77s\n",
      "============================================================\n"
     ]
    }
   ],
   "source": [
    "result = upload_preannotations(\n",
    "    project_id=project.project_id,\n",
    "    annotation_format='coco_json',\n",
    "    batch_annotation_dir=ANNOTATION_DIR\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8e13d066",
   "metadata": {},
   "source": [
    "## **5. Review SAM3 annotations on Labellerr**\n",
    "\n",
    "### When Pre-Annotations are loaded, you can now go to [Labellerr UI](https://www.labellerr.com/?utm_source=githubY&utm_medium=social&utm_campaign=github_clicks) to review and fix the annotations"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8390bae1",
   "metadata": {},
   "source": [
    "## **6. Export your annotations from Labellerr UI**\n",
    "\n",
    "![Labellerr](https://storage.googleapis.com/labellerr-cdn/%200%20Documentation-template/export_guide.webp)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "063ac2cc",
   "metadata": {},
   "source": [
    "## **7. Train Models with Exported COCO-JSON on Our CV Notebooks**\n",
    "\n",
    "Check out these hands-on notebooks to train YOLO models for your specific use case:\n",
    "\n",
    "- **[Fine-Tune your custom data on YOLOv12](https://github.com/Labellerr/Hands-On-Learning-in-Computer-Vision/blob/main/fine-tune%20YOLO%20for%20various%20use%20cases/Pill_Counting_Using_YOLOv12.ipynb)**  \n",
    "  Build an automated pill counting system for pharmaceutical and healthcare applications.\n",
    "\n",
    "- **[Intrusion Detection Using YOLOv8](https://github.com/Labellerr/Hands-On-Learning-in-Computer-Vision/blob/main/fine-tune%20YOLO%20for%20various%20use%20cases/Intrusion_detection_using_YOLO.ipynb)**  \n",
    "  Create a security system that detects unauthorized access in real-time.\n",
    "\n",
    "- **[Fine-Tune RT-DETR for Object Detection](https://github.com/Labellerr/Hands-On-Learning-in-Computer-Vision/blob/main/fine-tune%20YOLO%20for%20various%20use%20cases/Oil_Field_Monitoring_using_CV.ipynb)**  \n",
    "  Monitor and analyze vehicle traffic patterns for smart city applications.\n",
    "\n",
    "- **[Cell Counting with YOLO](https://github.com/Labellerr/Hands-On-Learning-in-Computer-Vision/blob/main/fine-tune%20YOLO%20for%20various%20use%20cases/Fine-Tune-YOLO-for-Cell-Counting.ipynb)**  \n",
    "  Automate biological cell counting for medical research and diagnostics.\n",
    "\n",
    "---\n",
    "\n",
    "## üßë‚Äçüî¨ Check Our Videos tutorials for Training\n",
    "\n",
    "Whether you're a beginner or a practitioner, our hands-on training videos are perfect for learning custom model building, computer vision techniques, and applied AI:\n",
    "\n",
    "- [How to Fine-Tune YOLO on Custom Dataset](https://www.youtube.com/watch?v=pBLWOe01QXU)  \n",
    "  Step-by-step guide to fine-tuning YOLO for real-world use‚Äîenvironment setup, annotation, training, validation, and inference.\n",
    "- [Build a Real-Time Intrusion Detection System with YOLO](https://www.youtube.com/watch?v=kwQeokYDVcE)  \n",
    "  Create an AI-powered system to detect intruders in real time using YOLO and computer vision.\n",
    "- [Finding Athlete Speed Using YOLO](https://www.youtube.com/watch?v=txW0CQe_pw0)  \n",
    "  Estimate real-time speed of athletes for sports analytics.\n",
    "- [Object Counting Using AI](https://www.youtube.com/watch?v=smsjBBQcIUQ)  \n",
    "  Learn dataset curation, annotation, and training for robust object counting AI applications.\n",
    "\n",
    "---\n",
    "\n",
    "## ü§ù Stay Connected\n",
    "\n",
    "- **Website:** [https://www.labellerr.com/](https://www.labellerr.com/)\n",
    "- **Blog:** [https://www.labellerr.com/blog/](https://www.labellerr.com/blog/)\n",
    "- **GitHub:** [Labellerr/Hands-On-Learning-in-Computer-Vision](https://github.com/Labellerr/Hands-On-Learning-in-Computer-Vision)\n",
    "- **LinkedIn:** [Labellerr](https://in.linkedin.com/company/labellerr)\n",
    "- **Twitter/X:** [@Labellerr1](https://x.com/Labellerr1)\n",
    "\n",
    "*Happy learning and building with Labellerr!*"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
